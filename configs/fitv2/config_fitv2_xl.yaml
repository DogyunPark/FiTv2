diffusion:
  pretrained_first_stage_model_path: stabilityai/sd-vae-ft-ema
  transport:
    path_type: Linear
    prediction: velocity
    loss_weight: null
    sample_eps: null
    train_eps: null
    snr_type: lognorm
  sampler:
    mode: ODE
    sde:
      sampling_method: Euler
      diffusion_form: sigma
      diffusion_norm: 1.0
      last_step: Mean
      last_step_size: 0.04
    ode:
      sampling_method: dopri5
      atol: 1.0e-06
      rtol: 0.001
      reverse: false
      likelihood: false
  network_config:
    target: fit.model.fit_model.FiT
    params:
      context_size: 256
      patch_size: 2
      in_channels: 4
      hidden_size: 1152
      depth: 36
      num_heads: 16
      mlp_ratio: 4.0
      class_dropout_prob: 0.1
      num_classes: 1000
      learn_sigma: false
      use_sit: true
      use_swiglu: true
      use_swiglu_large: false
      q_norm: layernorm
      k_norm: layernorm
      qk_norm_weight: false
      rel_pos_embed: rope
      abs_pos_embed: null
      adaln_type: lora
      adaln_lora_dim: 288

data:
  target: fit.data.in1k_latent_dataset.INLatentLoader
  params:
    train:
      data_path: datasets/imagenet1k_latents_256_sd_vae_ft_ema
      target_len: 256
      random: 'random'
      loader:
        batch_size: 16
        num_workers: 2
        shuffle: True


accelerate:
  # others
  gradient_accumulation_steps: 1
  mixed_precision: 'bf16'
  # training step config
  num_train_epochs: 
  max_train_steps: 2000000
  # optimizer config
  learning_rate: 1.0e-4
  learning_rate_base_batch_size: 256
  max_grad_norm: 1.0
  optimizer:
    target: torch.optim.AdamW
    params:
      betas: ${tuple:0.9, 0.999}
      weight_decay: 0 #1.0e-2
      eps: 1.0e-8
  lr_scheduler: constant_with_warmup
  lr_warmup_steps: 50000
  # checkpoint config
  logger: wandb
  checkpointing_epochs: False
  checkpointing_steps: 4000
  checkpointing_steps_list: [200000, 400000, 1000000, 1400000, 1500000, 1800000]
  checkpoints_total_limit: 2
  logging_steps: 1000
  