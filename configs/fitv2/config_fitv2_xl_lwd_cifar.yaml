diffusion:
  pretrained_first_stage_model_path: stabilityai/sd-vae-ft-ema
  transport:
    path_type: Linear
    prediction: velocity
    loss_weight: null
    sample_eps: null
    train_eps: null
    snr_type: lognorm
  sampler:
    mode: ODE
    sde:
      sampling_method: Euler
      diffusion_form: sigma
      diffusion_norm: 1.0
      last_step: Mean
      last_step_size: 0.04
    ode:
      sampling_method: dopri5
      atol: 1.0e-06
      rtol: 0.001
      reverse: false
      likelihood: false
  network_config:
    target: fit.model.fit_model.FiT
    params:
      context_size: 1024
      patch_size: 2
      in_channels: 4
      hidden_size: 2304
      depth: 40
      num_heads: 24
      mlp_ratio: 4.0
      class_dropout_prob: 0.1
      num_classes: 1000
      learn_sigma: false
      use_sit: true
      use_checkpoint: false
      use_swiglu: true
      use_swiglu_large: false
      q_norm: layernorm
      k_norm: layernorm
      qk_norm_weight: false
      rel_pos_embed: rope
      custom_freqs: ntk-aware
      decouple: true
      ori_max_pe_len: 16
      online_rope: true
      abs_pos_embed: null
      adaln_type: lora
      adaln_lora_dim: 576
      # pretrain_ckpt: ./checkpoints/FiTv2/FiTv2_3B_HRFT/model_ema.safetensors
      # ignore_keys: ['x_embedder', 'bias', 'LN', 'final_layer']
      # finetune: partial
  
  distillation_network_config:
    target: fit.model.fit_model_lwd.FiTLwD
    params:
      context_size: 256
      patch_size: 2
      in_channels: 3
      hidden_size: 384
      depth: 30
      num_heads: 6
      mlp_ratio: 4.0
      class_dropout_prob: 0.1
      num_classes: 10
      learn_sigma: false
      use_sit: true
      use_swiglu: false
      use_swiglu_large: false
      use_checkpoint: false
      q_norm: layernorm
      k_norm: layernorm
      qk_norm_weight: false
      rel_pos_embed: rope
      abs_pos_embed: null
      adaln_type: normal
      adaln_lora_dim: 288
      number_of_perflow: 6
      fourier_basis: false
      edm_sigmas: false
      perlayer_embedder: false
      overlap: false
      number_of_shared_blocks: 2
      number_of_representation_blocks: 2
      max_cached_len: 256
  gan_guidance_config:
    sat: false
    params:
      disc_num_layers: 3
      disc_in_channels: 3
      disc_weight: 1.0
      disc_ndf: 64
      disc_loss: "hinge"

data:
  target: fit.data.in1k_latent_dataset.INLatentLoader
  params:
    train:
      data_path: datasets/imagenet1k_latents_256_sd_vae_ft_ema
      target_len: 256
      random: 'random'
      loader:
        batch_size: 12
        num_workers: 4
        shuffle: True


accelerate:
  # others
  gradient_accumulation_steps: 1
  mixed_precision: 'fp16'
  # training step config
  num_train_epochs: 
  max_train_steps: 2000000
  # optimizer config
  learning_rate: 1.0e-4
  learning_rate_base_batch_size: 256
  max_grad_norm: 1.0
  optimizer:
    target: torch.optim.AdamW
    #target: CAME
    params:
      betas: ${tuple:0.9, 0.999}
      weight_decay: 1.0e-2
      eps: 1.0e-8
      # betas: ${tuple:0.9, 0.999, 0.9999}
      # weight_decay: 1.0e-2
      # eps: ${tuple:1e-30, 1e-16}
  lr_scheduler: constant_with_warmup
  lr_warmup_steps: 0
  # checkpoint config
  logger: wandb
  checkpointing_epochs: False
  checkpointing_steps: 5000
  checkpointing_steps_list: [500, 200000, 400000, 1000000, 1400000, 1500000, 1800000]
  checkpoints_total_limit: 2
  logging_steps: 1000
  evaluation_steps: 200
  eval_fid_steps: 5000

  # fsdp_config:
  #   sharding_strategy: FULL_SHARD
  #   backward_prefetch: BACKWARD_PRE
  #   min_num_params: 10000000
  #   cpu_offload: False
  #   state_dict_type: FULL_STATE_DICT
  #   limit_all_gathers: false
  #   use_orig_params: true
  #   sync_module_states: true
  #   forward_prefetch: false
  #   activation_checkpointing: false
