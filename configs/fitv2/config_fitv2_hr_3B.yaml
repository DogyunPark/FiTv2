diffusion:
  pretrained_first_stage_model_path: stabilityai/sd-vae-ft-ema
  transport:
    path_type: Linear
    prediction: velocity
    loss_weight: null
    sample_eps: null
    train_eps: null
    snr_type: lognorm
  sampler:
    mode: ODE
    sde:
      sampling_method: Euler
      diffusion_form: sigma
      diffusion_norm: 1.0
      last_step: Mean
      last_step_size: 0.04
    ode:
      sampling_method: dopri5
      atol: 1.0e-06
      rtol: 0.001
      reverse: false
      likelihood: false
  network_config:
    target: fit.model.fit_model.FiT
    params:
      context_size: 1024
      patch_size: 2
      in_channels: 4
      hidden_size: 2304
      depth: 40
      num_heads: 24
      mlp_ratio: 4.0
      class_dropout_prob: 0.1
      num_classes: 1000
      learn_sigma: false
      use_sit: true
      use_checkpoint: true
      use_swiglu: true
      use_swiglu_large: false
      q_norm: layernorm
      k_norm: layernorm
      qk_norm_weight: false
      rel_pos_embed: rope
      custom_freqs: ntk-aware
      decouple: true
      ori_max_pe_len: 16
      online_rope: true
      abs_pos_embed: null
      adaln_type: lora
      adaln_lora_dim: 576
      pretrain_ckpt: ./checkpoints/FiTv2/FiTv2_3B_HRFT/model_ema.safetensors
      ignore_keys: ['x_embedder', 'bias', 'LN', 'final_layer']
      finetune: partial
      
data:
  target: fit.data.in1k_latent_dataset.INLatentLoader
  params:
    train:
      data_path: datasets/imagenet1k_latents_1024_sd_vae_ft_ema
      target_len: 1024
      random: 'random'
      loader:
        batch_size: 16
        num_workers: 2
        shuffle: True


accelerate:
  # others
  gradient_accumulation_steps: 1
  mixed_precision: 'bf16'
  # training step config
  num_train_epochs: 
  max_train_steps: 200000
  # optimizer config
  learning_rate: 1.0e-4
  learning_rate_base_batch_size: 256
  max_grad_norm: 1.0
  optimizer:
    target: torch.optim.AdamW
    params:
      betas: ${tuple:0.9, 0.999}
      weight_decay: 0 #1.0e-2
      eps: 1.0e-8
  lr_scheduler: constant
  lr_warmup_steps: 0
  # checkpoint config
  logger: wandb
  checkpointing_epochs: False
  checkpointing_steps: 4000
  checkpointing_steps_list: [40000, 80000, 100000]
  checkpoints_total_limit: 2
  logging_steps: 1000
