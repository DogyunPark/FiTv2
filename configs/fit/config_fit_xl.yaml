diffusion:
  pretrained_first_stage_model_path: stabilityai/sd-vae-ft-ema
  improved_diffusion:
    timestep_respacing: ''
    noise_schedule: linear
    use_kl: false
    sigma_small: false
    predict_xstart: false
    learn_sigma: true
    rescale_learned_sigmas: false
    diffusion_steps: 1000
  noise_scheduler:
    num_train_timesteps: 1000
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: linear
    prediction_type: epsilon
    steps_offset: 0
    clip_sample: false
  network_config:
    target: fit.model.fit_model.FiT
    params:
      context_size: 256
      patch_size: 2
      in_channels: 4
      hidden_size: 1152
      depth: 28
      num_heads: 16
      mlp_ratio: 4.0
      class_dropout_prob: 0.1
      num_classes: 1000
      learn_sigma: true
      use_swiglu: true
      use_swiglu_large: true
      rel_pos_embed: rope


data:
  target: fit.data.in1k_latent_dataset.INLatentLoader
  params:
    train:
      data_path: datasets/imagenet1k_latents_256_sd_vae_ft_ema
      target_len: 256
      random: 'resize'
      loader:
        batch_size: 32
        num_workers: 2
        shuffle: True

accelerate:
  # others
  gradient_accumulation_steps: 1
  mixed_precision: 'bf16'
  # training step config
  num_train_epochs: 
  max_train_steps: 2000000
  # optimizer config
  learning_rate: 1.0e-4
  learning_rate_base_batch_size: 256
  max_grad_norm: 1.0
  optimizer:
    target: torch.optim.AdamW
    params:
      betas: ${tuple:0.9, 0.999}
      weight_decay: 0 #1.0e-2
      eps: 1.0e-8
  lr_scheduler: constant
  lr_warmup_steps: 500
  # checkpoint config
  logger: wandb
  checkpointing_epochs: False
  checkpointing_steps: 100000
  checkpointing_steps_list: [400000, 1000000, 2000000]
  checkpoints_total_limit: 2
  logging_steps: 10000
